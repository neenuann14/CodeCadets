# -*- coding: utf-8 -*-
"""chatupdated.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_NIQt2RECK1M1qyvsDi8YiXn9CxBb5YE
"""

# Step 1: Install necessary packages
#!pip install openvino
#!pip install optimum[openvino]
#!pip install nncf
#!pip install transformers
#!pip install torch

# Step 2: Download and convert the model
import torch
from transformers import AutoTokenizer, BlenderbotForConditionalGeneration

model_name = "facebook/blenderbot-400M-distill"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = BlenderbotForConditionalGeneration.from_pretrained(model_name)

import os

# Tokenize input text
dummy_input = tokenizer("Hello, how are you?", return_tensors="pt")
print("input shape", dummy_input['input_ids'].shape)

# Generate attention mask with correct shape
dummy_attention_mask = torch.ones(dummy_input['input_ids'].shape, dtype=torch.long)

# Create dummy decoder inputs
dummy_decoder_input_ids = torch.ones((1, 1), dtype=torch.long)  # Example dummy input for the decoder
dummy_decoder_attention_mask = torch.ones(dummy_decoder_input_ids.shape, dtype=torch.long)

# Specify output directory and create it if it doesn't exist
output_dir = "blenderbot_openvino_ir"
os.makedirs(output_dir, exist_ok=True)

# Specify the output file path
output_path = os.path.join(output_dir, "blenderbot.onnx")

# Export the model to ONNX format
torch.onnx.export(
    model,
    (dummy_input['input_ids'], dummy_attention_mask, dummy_decoder_input_ids, dummy_decoder_attention_mask),
    output_path,
    opset_version=11,
    input_names=["input_ids", "attention_mask", "decoder_input_ids", "decoder_attention_mask"],
    output_names=["output"]
)

#!pip install openvino-dev

import openvino.runtime as ov
# Load the ONNX model
core = ov.Core()
onnx_model = core.read_model("/content/blenderbot_openvino_ir/blenderbot.onnx")

# Specify the output paths for the OpenVINO IR format files
ir_xml_path = os.path.join(output_dir, "blenderbot_ir.xml")
ir_bin_path = os.path.join(output_dir, "blenderbot_ir.bin")

# Convert the ONNX model to OpenVINO IR format
# Note: No need to compile the model for conversion
ov.serialize(onnx_model, ir_xml_path, ir_bin_path)

print("Model has been successfully converted to OpenVINO IR format and saved to", output_dir)

from nncf import NNCFConfig
from nncf.quantization import quantize, QuantizationPreset
import openvino.runtime as ov
from torch.utils.data import Dataset, DataLoader
import torch
import os

# Verify the existence of the model file
model_dir = "blenderbot_openvino_ir"
model_path = os.path.join(model_dir, "blenderbot_ir.xml")
bin_path = os.path.join(model_dir, "blenderbot_ir.bin")

if not os.path.exists(model_path):
    raise FileNotFoundError(f"Model file not found: {model_path}")
if not os.path.exists(bin_path):
    raise FileNotFoundError(f"Weights file not found: {bin_path}")

# Load the OpenVINO model
core = ov.Core()
ov_model = core.read_model(model_path, bin_path)

# NNCF Configuration
nncf_config = NNCFConfig.from_dict({
    "input_info": {
        "sample_size": [1, 7]  # Adjust to the correct input shape for your model
    },
    "compression": {
        "algorithm": "quantization",
        "initializer": {
            "range": {
                "num_init_samples": 256
            },
            "batchnorm_adaptation": {
                "num_bn_adaptation_samples": 256
            }
        }
    }
})

# Create a dummy calibration dataset
class CalibrationDataset(Dataset):
    def __init__(self, num_samples, batch_size=1):
        self.num_samples = num_samples
        self.batch_size = batch_size

    def __len__(self):
        return self.num_samples

    def __getitem__(self, item):
        # Generate a tensor with the correct shape [1, 7]
        input_ids = torch.randint(0, 100, (1, 7), dtype=torch.int32)  # Ensure the shape is [1, 7]
        attention_mask = torch.ones((1, 7), dtype=torch.int32)        # Ensure the shape is [1, 7]
        return {"input_ids": input_ids, "attention_mask": attention_mask}

    def get_batch_size(self):
        return self.batch_size

    def get_length(self):
        return self.__len__()

    def get_inference_data(self):
        dataloader = DataLoader(self, batch_size=self.batch_size)
        for batch in dataloader:
            input_ids = batch["input_ids"].squeeze(0)
            attention_mask = batch["attention_mask"].squeeze(0)
            yield {"input_ids": input_ids, "attention_mask": attention_mask}

calibration_dataset = CalibrationDataset(num_samples=256, batch_size=1)

# Try to explicitly set the backend
from nncf.common.utils.backend import BackendType
from nncf.common.utils.backend import get_backend

backend = get_backend(ov_model)

if backend == BackendType.OPENVINO:
    print("Backend successfully set to OpenVINO")
else:
    raise RuntimeError("Backend is not set to OpenVINO. Current backend: ", backend)

# Quantize the OpenVINO model
compressed_model = quantize(ov_model, calibration_dataset, preset=QuantizationPreset.MIXED)

# Specify the output paths for the quantized model files
quantized_model_xml_path = os.path.join(output_dir, "blenderbot_quantized.xml")
quantized_model_bin_path = os.path.join(output_dir, "blenderbot_quantized.bin")

# Save the quantized model
ov.serialize(compressed_model, quantized_model_xml_path, quantized_model_bin_path)

print("Quantized OpenVINO model saved as blenderbot_quantized.xml and blenderbot_quantized.bin in the directory:", output_dir)

from transformers import BlenderbotForConditionalGeneration, BlenderbotTokenizer

# Load the pretrained model and tokenizer
model_name = "facebook/blenderbot-400M-distill"
model = BlenderbotForConditionalGeneration.from_pretrained(model_name)
tokenizer = BlenderbotTokenizer.from_pretrained(model_name)

# Save the model and tokenizer configuration files
model.save_pretrained("blenderbot_openvino_ir")
tokenizer.save_pretrained("blenderbot_openvino_ir")

# Initialize the pipeline with the quantized model
from transformers import pipeline

model_dir = "blenderbot_openvino_ir"
chat_pipeline = pipeline("text2text-generation", model=model_dir, tokenizer=model_dir)

# Step 4: Implement the chatbot
def chat_with_bot(user_input):
    result = chat_pipeline(user_input, max_length=100)
    return result[0]['generated_text']

def chat():
    print("Welcome to the Chatbot. Type 'exit' or 'quit' to end the conversation.")
    while True:
        user_input = input("You: ")
        if user_input.lower() in ["exit", "quit","bye","goodbye"]:
            print("Chatbot: Goodbye!")
            break
        response = chat_with_bot(user_input)
        print(f"Chatbot: {response}")


chat()






